# Zhang Ziang (å¼ æ¢“æ˜‚)

<p align="center">
  <img src="https://komarev.com/ghpvc/?username=SpatialVision&label=Profile%20views&color=0e75b6&style=flat" alt="Views" />
  <a href="mailto:ziangzhang_vsama@qq.com"><img src="https://img.shields.io/badge/Email-ziangzhang__vsama%40qq.com-blue?logo=gmail" alt="Email"></a>
</p>

### ðŸŒŸ About Me
I am a researcher/developer focused on **Spatial Perception** and **Foundation Models**. My work aims to bridge the gap between multimodal learning and 3D geometric understanding.

- ðŸ”­ **Focus**: Spatial Perception and Understanding.  
- ðŸŒ± **Learning**: Flow-matching, 3D Gaussian Splatting, Advanced 3D Representations.
- ðŸ‘¯ **Collaborate**: Computer Vision, Multimodal LLMs (MLLM), Foundation Models.
- ðŸ’¬ **Ask me about**: CV, PEFT, or 3D geometry in MLLMs.

---

### ðŸ”¥ Featured Work: Orient-Anything Series

<table border="0">
  <tr>
    <td width="50%">
      <p align="center"><b>Orient-Anything-V2 (NeurIPS 2025 Spotlight)</b></p>
      <a href="https://github.com/SpatialVision/Orient-Anything-V2">
        <img src="https://raw.githubusercontent.com/SpatialVision/Orient-Anything-V2/main/assets/overview.jpg" width="100%" alt="OriAnyV2 Overview">
      </a>
    </td>
    <td width="50%">
      <p align="center"><b>Orient-Anything (ICML 2025)</b></p>
      <a href="https://github.com/SpatialVision/Orient-Anything">
        <img src="https://raw.githubusercontent.com/SpatialVision/Orient-Anything/main/assets/demo.png" width="100%" alt="OriAny Demo">
      </a>
    </td>
  </tr>
</table>

---

### ðŸ›  Tech Stack
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)
![C++](https://img.shields.io/badge/c++-%2300599C.svg?style=for-the-badge&logo=c%2B%2B&logoColor=white)
![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)

---

### ðŸ“š Selected Publications

| Project | Conference | Highlights |
| :--- | :--- | :--- |
| **[Orient-Anything-V2](https://github.com/SpatialVision/Orient-Anything-V2)** | **NeurIPS 2025 Spotlight** | Unifying Orientation and Rotation Understanding |
| **[Orient-Anything](https://github.com/SpatialVision/Orient-Anything)** | **ICML 2025** | Robust Object Orientation Estimation |
| **[DSI-Bench](https://github.com/SpatialVision/dsibench)** | arXiv 2025 | A Benchmark for Spatial Intelligence |
| **[OmniBind](https://github.com/zehanwang01/OmniBind)** | **ICLR 2025** | Multi-modal Binding Foundation Models |
| **[FreeBind](https://github.com/zehanwang01/FreeBind)** | **ICML 2024** | Flexible Modality Alignment |
| **[Ex-MCR](https://github.com/MCR-PEFT/Ex-MCR)** | **NeurIPS 2024** | Efficient Multi-modal Learning & PEFT |
